{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
    "\n",
    "from conf_finetune import _C as cfg\n",
    "from main_finetune import initialize_data_loader, initialize_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "\n",
    "import datasets\n",
    "import models\n",
    "import utils.transforms\n",
    "from utils.lr_decay import param_groups_lrd\n",
    "from utils.sampler import RandomIdentitySampler\n",
    "from utils.scaler import NativeScalerWithGradNormCount\n",
    "from utils.triplet_loss import TripletLoss\n",
    "\n",
    "def initialize_model(cfg, num_classes, device_id):\n",
    "    # logger.info(f'creating model: {cfg.MODEL.NAME}')\n",
    "    model = models.__dict__[cfg.MODEL.NAME](cfg, num_classes)\n",
    "    model.cuda(device_id)\n",
    "    \n",
    "    triplet = TripletLoss()\n",
    "    def loss_func(feats, logits, target):\n",
    "        if not isinstance(feats, tuple) and not isinstance(logits, tuple):\n",
    "            id_loss = F.cross_entropy(logits, target)\n",
    "            tri_loss = triplet(feats, target)[0]\n",
    "        else:\n",
    "            id_loss = [F.cross_entropy(logit, target) for logit in logits]\n",
    "            id_loss = sum(id_loss) / len(id_loss)\n",
    "            tri_loss = [triplet(feat, target)[0] for feat in feats]\n",
    "            tri_loss = sum(tri_loss) / len(tri_loss)\n",
    "        return cfg.MODEL.ID_LOSS_WEIGHT * id_loss + cfg.MODEL.TRI_LOSS_WEIGHT * tri_loss, id_loss, tri_loss\n",
    "    \n",
    "    param_groups = param_groups_lrd(model, cfg.OPTIMIZER.WEIGHT_DECAY, model.no_weight_decay(), cfg.OPTIMIZER.LAYER_DECAY)\n",
    "    optimizer = torch.optim.AdamW(param_groups, cfg.OPTIMIZER.LR, cfg.OPTIMIZER.BETAS)\n",
    "    scaler = NativeScalerWithGradNormCount()\n",
    "    return model, loss_func, optimizer, scaler\n",
    "\n",
    "\n",
    "def initialize_data_loader(cfg):\n",
    "    train_transform = utils.transforms.__dict__[cfg.INPUT.TRANSFORM](cfg)\n",
    "    train_dataset = datasets.__dict__[cfg.DATASET.NAME](cfg, train_transform, is_train=True)\n",
    "    num_classes = train_dataset.num_classes\n",
    "    train_sampler = RandomIdentitySampler(train_dataset, cfg.ENGINE.BATCH_SIZE, cfg.DATALOADER.NUM_INSTANCES)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg.ENGINE.BATCH_SIZE, \n",
    "        num_workers=cfg.DATALOADER.NUM_WORKERS, \n",
    "        pin_memory=True, \n",
    "        sampler=train_sampler\n",
    "    )\n",
    "\n",
    "    val_transform = utils.transforms.__dict__[cfg.VALIDATE.TRANSFORM](cfg)\n",
    "    val_dataset = datasets.__dict__[cfg.DATASET.NAME](cfg, val_transform, is_train=False)\n",
    "    num_queries = val_dataset.num_queries\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=cfg.VALIDATE.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.DATALOADER.NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, num_classes, num_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Antelope fine-tuning')\n",
    "parser.add_argument('--config_file', default='', help='path to config file', type=str)\n",
    "parser.add_argument('opts', help='modify config options using the command-line', default=None, nargs=argparse.REMAINDER)\n",
    "args = parser.parse_args(args=['--config_file', 'configs/finetune/MSMT17/mae_inet_lup_vitb_ep800_ratio_optimized/baseline.yaml'])\n",
    "cfg.merge_from_file(args.config_file)\n",
    "cfg.merge_from_list(args.opts)\n",
    "cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, num_classes, num_queries = initialize_data_loader(cfg)\n",
    "model, criterion, optimizer, scaler = initialize_model(cfg, num_classes, device_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyz/miniconda3/lib/python3.9/site-packages/timm/models/vision_transformer.py:202: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
      "Unsupported operator aten::add encountered 25 time(s)\n",
      "Unsupported operator aten::div encountered 12 time(s)\n",
      "Unsupported operator aten::mul encountered 34 time(s)\n",
      "Unsupported operator aten::softmax encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::bernoulli_ encountered 22 time(s)\n",
      "Unsupported operator aten::div_ encountered 22 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "726526623744\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "flops = FlopCountAnalysis(model, batch[0].cuda())\n",
    "print(flops.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| name                       | #elements or shape   |\n",
      "|:---------------------------|:---------------------|\n",
      "| model                      | 86.5M                |\n",
      "|  cls_token                 |  (1, 1, 768)         |\n",
      "|  pos_embed                 |  (1, 129, 768)       |\n",
      "|  patch_embed               |  0.6M                |\n",
      "|   patch_embed.proj         |   0.6M               |\n",
      "|    patch_embed.proj.weight |    (768, 3, 16, 16)  |\n",
      "|    patch_embed.proj.bias   |    (768,)            |\n",
      "|  blocks                    |  85.1M               |\n",
      "|   blocks.0                 |   7.1M               |\n",
      "|    blocks.0.norm1          |    1.5K              |\n",
      "|    blocks.0.attn           |    2.4M              |\n",
      "|    blocks.0.norm2          |    1.5K              |\n",
      "|    blocks.0.mlp            |    4.7M              |\n",
      "|   blocks.1                 |   7.1M               |\n",
      "|    blocks.1.norm1          |    1.5K              |\n",
      "|    blocks.1.attn           |    2.4M              |\n",
      "|    blocks.1.norm2          |    1.5K              |\n",
      "|    blocks.1.mlp            |    4.7M              |\n",
      "|   blocks.2                 |   7.1M               |\n",
      "|    blocks.2.norm1          |    1.5K              |\n",
      "|    blocks.2.attn           |    2.4M              |\n",
      "|    blocks.2.norm2          |    1.5K              |\n",
      "|    blocks.2.mlp            |    4.7M              |\n",
      "|   blocks.3                 |   7.1M               |\n",
      "|    blocks.3.norm1          |    1.5K              |\n",
      "|    blocks.3.attn           |    2.4M              |\n",
      "|    blocks.3.norm2          |    1.5K              |\n",
      "|    blocks.3.mlp            |    4.7M              |\n",
      "|   blocks.4                 |   7.1M               |\n",
      "|    blocks.4.norm1          |    1.5K              |\n",
      "|    blocks.4.attn           |    2.4M              |\n",
      "|    blocks.4.norm2          |    1.5K              |\n",
      "|    blocks.4.mlp            |    4.7M              |\n",
      "|   blocks.5                 |   7.1M               |\n",
      "|    blocks.5.norm1          |    1.5K              |\n",
      "|    blocks.5.attn           |    2.4M              |\n",
      "|    blocks.5.norm2          |    1.5K              |\n",
      "|    blocks.5.mlp            |    4.7M              |\n",
      "|   blocks.6                 |   7.1M               |\n",
      "|    blocks.6.norm1          |    1.5K              |\n",
      "|    blocks.6.attn           |    2.4M              |\n",
      "|    blocks.6.norm2          |    1.5K              |\n",
      "|    blocks.6.mlp            |    4.7M              |\n",
      "|   blocks.7                 |   7.1M               |\n",
      "|    blocks.7.norm1          |    1.5K              |\n",
      "|    blocks.7.attn           |    2.4M              |\n",
      "|    blocks.7.norm2          |    1.5K              |\n",
      "|    blocks.7.mlp            |    4.7M              |\n",
      "|   blocks.8                 |   7.1M               |\n",
      "|    blocks.8.norm1          |    1.5K              |\n",
      "|    blocks.8.attn           |    2.4M              |\n",
      "|    blocks.8.norm2          |    1.5K              |\n",
      "|    blocks.8.mlp            |    4.7M              |\n",
      "|   blocks.9                 |   7.1M               |\n",
      "|    blocks.9.norm1          |    1.5K              |\n",
      "|    blocks.9.attn           |    2.4M              |\n",
      "|    blocks.9.norm2          |    1.5K              |\n",
      "|    blocks.9.mlp            |    4.7M              |\n",
      "|   blocks.10                |   7.1M               |\n",
      "|    blocks.10.norm1         |    1.5K              |\n",
      "|    blocks.10.attn          |    2.4M              |\n",
      "|    blocks.10.norm2         |    1.5K              |\n",
      "|    blocks.10.mlp           |    4.7M              |\n",
      "|   blocks.11                |   7.1M               |\n",
      "|    blocks.11.norm1         |    1.5K              |\n",
      "|    blocks.11.attn          |    2.4M              |\n",
      "|    blocks.11.norm2         |    1.5K              |\n",
      "|    blocks.11.mlp           |    4.7M              |\n",
      "|  norm                      |  1.5K                |\n",
      "|   norm.weight              |   (768,)             |\n",
      "|   norm.bias                |   (768,)             |\n",
      "|  head                      |  0.8M                |\n",
      "|   head.weight              |   (1041, 768)        |\n",
      "|   head.bias                |   (1041,)            |\n"
     ]
    }
   ],
   "source": [
    "print(parameter_count_table(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Antelope fine-tuning')\n",
    "parser.add_argument('--config_file', default='', help='path to config file', type=str)\n",
    "parser.add_argument('opts', help='modify config options using the command-line', default=None, nargs=argparse.REMAINDER)\n",
    "args = parser.parse_args(args=['--config_file', 'configs/finetune/MSMT17/mae_inet_lup_vitb_ep800_ratio_optimized/lem_pool.yaml'])\n",
    "cfg.merge_from_file(args.config_file)\n",
    "cfg.merge_from_list(args.opts)\n",
    "cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, num_classes, num_queries = initialize_data_loader(cfg)\n",
    "model, criterion, optimizer, scaler = initialize_model(cfg, num_classes, device_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyz/miniconda3/lib/python3.9/site-packages/timm/models/vision_transformer.py:202: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
      "Unsupported operator aten::add encountered 25 time(s)\n",
      "Unsupported operator aten::div encountered 12 time(s)\n",
      "Unsupported operator aten::mul encountered 34 time(s)\n",
      "Unsupported operator aten::softmax encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::bernoulli_ encountered 22 time(s)\n",
      "Unsupported operator aten::div_ encountered 22 time(s)\n",
      "Unsupported operator aten::mean encountered 1 time(s)\n",
      "Unsupported operator aten::add_ encountered 2 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "726578528256\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "flops = FlopCountAnalysis(model, batch[0].cuda())\n",
    "print(flops.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| name                       | #elements or shape   |\n",
      "|:---------------------------|:---------------------|\n",
      "| model                      | 87.4M                |\n",
      "|  cls_token                 |  (1, 1, 768)         |\n",
      "|  pos_embed                 |  (1, 129, 768)       |\n",
      "|  patch_embed               |  0.6M                |\n",
      "|   patch_embed.proj         |   0.6M               |\n",
      "|    patch_embed.proj.weight |    (768, 3, 16, 16)  |\n",
      "|    patch_embed.proj.bias   |    (768,)            |\n",
      "|  blocks                    |  85.1M               |\n",
      "|   blocks.0                 |   7.1M               |\n",
      "|    blocks.0.norm1          |    1.5K              |\n",
      "|    blocks.0.attn           |    2.4M              |\n",
      "|    blocks.0.norm2          |    1.5K              |\n",
      "|    blocks.0.mlp            |    4.7M              |\n",
      "|   blocks.1                 |   7.1M               |\n",
      "|    blocks.1.norm1          |    1.5K              |\n",
      "|    blocks.1.attn           |    2.4M              |\n",
      "|    blocks.1.norm2          |    1.5K              |\n",
      "|    blocks.1.mlp            |    4.7M              |\n",
      "|   blocks.2                 |   7.1M               |\n",
      "|    blocks.2.norm1          |    1.5K              |\n",
      "|    blocks.2.attn           |    2.4M              |\n",
      "|    blocks.2.norm2          |    1.5K              |\n",
      "|    blocks.2.mlp            |    4.7M              |\n",
      "|   blocks.3                 |   7.1M               |\n",
      "|    blocks.3.norm1          |    1.5K              |\n",
      "|    blocks.3.attn           |    2.4M              |\n",
      "|    blocks.3.norm2          |    1.5K              |\n",
      "|    blocks.3.mlp            |    4.7M              |\n",
      "|   blocks.4                 |   7.1M               |\n",
      "|    blocks.4.norm1          |    1.5K              |\n",
      "|    blocks.4.attn           |    2.4M              |\n",
      "|    blocks.4.norm2          |    1.5K              |\n",
      "|    blocks.4.mlp            |    4.7M              |\n",
      "|   blocks.5                 |   7.1M               |\n",
      "|    blocks.5.norm1          |    1.5K              |\n",
      "|    blocks.5.attn           |    2.4M              |\n",
      "|    blocks.5.norm2          |    1.5K              |\n",
      "|    blocks.5.mlp            |    4.7M              |\n",
      "|   blocks.6                 |   7.1M               |\n",
      "|    blocks.6.norm1          |    1.5K              |\n",
      "|    blocks.6.attn           |    2.4M              |\n",
      "|    blocks.6.norm2          |    1.5K              |\n",
      "|    blocks.6.mlp            |    4.7M              |\n",
      "|   blocks.7                 |   7.1M               |\n",
      "|    blocks.7.norm1          |    1.5K              |\n",
      "|    blocks.7.attn           |    2.4M              |\n",
      "|    blocks.7.norm2          |    1.5K              |\n",
      "|    blocks.7.mlp            |    4.7M              |\n",
      "|   blocks.8                 |   7.1M               |\n",
      "|    blocks.8.norm1          |    1.5K              |\n",
      "|    blocks.8.attn           |    2.4M              |\n",
      "|    blocks.8.norm2          |    1.5K              |\n",
      "|    blocks.8.mlp            |    4.7M              |\n",
      "|   blocks.9                 |   7.1M               |\n",
      "|    blocks.9.norm1          |    1.5K              |\n",
      "|    blocks.9.attn           |    2.4M              |\n",
      "|    blocks.9.norm2          |    1.5K              |\n",
      "|    blocks.9.mlp            |    4.7M              |\n",
      "|   blocks.10                |   7.1M               |\n",
      "|    blocks.10.norm1         |    1.5K              |\n",
      "|    blocks.10.attn          |    2.4M              |\n",
      "|    blocks.10.norm2         |    1.5K              |\n",
      "|    blocks.10.mlp           |    4.7M              |\n",
      "|   blocks.11                |   7.1M               |\n",
      "|    blocks.11.norm1         |    1.5K              |\n",
      "|    blocks.11.attn          |    2.4M              |\n",
      "|    blocks.11.norm2         |    1.5K              |\n",
      "|    blocks.11.mlp           |    4.7M              |\n",
      "|  norm                      |  1.5K                |\n",
      "|   norm.weight              |   (768,)             |\n",
      "|   norm.bias                |   (768,)             |\n",
      "|  head                      |  0.8M                |\n",
      "|   head.weight              |   (1041, 768)        |\n",
      "|   head.bias                |   (1041,)            |\n",
      "|  pool_norm                 |  1.5K                |\n",
      "|   pool_norm.weight         |   (768,)             |\n",
      "|   pool_norm.bias           |   (768,)             |\n",
      "|  pool_fc                   |  0.8M                |\n",
      "|   pool_fc.weight           |   (1041, 768)        |\n",
      "|   pool_fc.bias             |   (1041,)            |\n",
      "|  pool_bn                   |  1.5K                |\n",
      "|   pool_bn.weight           |   (768,)             |\n",
      "|   pool_bn.bias             |   (768,)             |\n",
      "|  bn                        |  1.5K                |\n",
      "|   bn.weight                |   (768,)             |\n",
      "|   bn.bias                  |   (768,)             |\n"
     ]
    }
   ],
   "source": [
    "print(parameter_count_table(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Antelope fine-tuning')\n",
    "parser.add_argument('--config_file', default='', help='path to config file', type=str)\n",
    "parser.add_argument('opts', help='modify config options using the command-line', default=None, nargs=argparse.REMAINDER)\n",
    "args = parser.parse_args(args=['--config_file', 'configs/finetune/MSMT17/mae_inet_lup_vitb_ep800_ratio_optimized/lem_tran.yaml'])\n",
    "cfg.merge_from_file(args.config_file)\n",
    "cfg.merge_from_list(args.opts)\n",
    "cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, num_classes, num_queries = initialize_data_loader(cfg)\n",
    "model, criterion, optimizer, scaler = initialize_model(cfg, num_classes, device_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyz/miniconda3/lib/python3.9/site-packages/timm/models/vision_transformer.py:202: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
      "Unsupported operator aten::add encountered 27 time(s)\n",
      "Unsupported operator aten::div encountered 13 time(s)\n",
      "Unsupported operator aten::mul encountered 37 time(s)\n",
      "Unsupported operator aten::softmax encountered 13 time(s)\n",
      "Unsupported operator aten::gelu encountered 13 time(s)\n",
      "Unsupported operator aten::bernoulli_ encountered 24 time(s)\n",
      "Unsupported operator aten::div_ encountered 24 time(s)\n",
      "Unsupported operator aten::mean encountered 1 time(s)\n",
      "Unsupported operator aten::add_ encountered 2 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786234114048\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "flops = FlopCountAnalysis(model, batch[0].cuda())\n",
    "print(flops.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| name                       | #elements or shape   |\n",
      "|:---------------------------|:---------------------|\n",
      "| model                      | 94.4M                |\n",
      "|  cls_token                 |  (1, 1, 768)         |\n",
      "|  pos_embed                 |  (1, 129, 768)       |\n",
      "|  patch_embed               |  0.6M                |\n",
      "|   patch_embed.proj         |   0.6M               |\n",
      "|    patch_embed.proj.weight |    (768, 3, 16, 16)  |\n",
      "|    patch_embed.proj.bias   |    (768,)            |\n",
      "|  blocks                    |  85.1M               |\n",
      "|   blocks.0                 |   7.1M               |\n",
      "|    blocks.0.norm1          |    1.5K              |\n",
      "|    blocks.0.attn           |    2.4M              |\n",
      "|    blocks.0.norm2          |    1.5K              |\n",
      "|    blocks.0.mlp            |    4.7M              |\n",
      "|   blocks.1                 |   7.1M               |\n",
      "|    blocks.1.norm1          |    1.5K              |\n",
      "|    blocks.1.attn           |    2.4M              |\n",
      "|    blocks.1.norm2          |    1.5K              |\n",
      "|    blocks.1.mlp            |    4.7M              |\n",
      "|   blocks.2                 |   7.1M               |\n",
      "|    blocks.2.norm1          |    1.5K              |\n",
      "|    blocks.2.attn           |    2.4M              |\n",
      "|    blocks.2.norm2          |    1.5K              |\n",
      "|    blocks.2.mlp            |    4.7M              |\n",
      "|   blocks.3                 |   7.1M               |\n",
      "|    blocks.3.norm1          |    1.5K              |\n",
      "|    blocks.3.attn           |    2.4M              |\n",
      "|    blocks.3.norm2          |    1.5K              |\n",
      "|    blocks.3.mlp            |    4.7M              |\n",
      "|   blocks.4                 |   7.1M               |\n",
      "|    blocks.4.norm1          |    1.5K              |\n",
      "|    blocks.4.attn           |    2.4M              |\n",
      "|    blocks.4.norm2          |    1.5K              |\n",
      "|    blocks.4.mlp            |    4.7M              |\n",
      "|   blocks.5                 |   7.1M               |\n",
      "|    blocks.5.norm1          |    1.5K              |\n",
      "|    blocks.5.attn           |    2.4M              |\n",
      "|    blocks.5.norm2          |    1.5K              |\n",
      "|    blocks.5.mlp            |    4.7M              |\n",
      "|   blocks.6                 |   7.1M               |\n",
      "|    blocks.6.norm1          |    1.5K              |\n",
      "|    blocks.6.attn           |    2.4M              |\n",
      "|    blocks.6.norm2          |    1.5K              |\n",
      "|    blocks.6.mlp            |    4.7M              |\n",
      "|   blocks.7                 |   7.1M               |\n",
      "|    blocks.7.norm1          |    1.5K              |\n",
      "|    blocks.7.attn           |    2.4M              |\n",
      "|    blocks.7.norm2          |    1.5K              |\n",
      "|    blocks.7.mlp            |    4.7M              |\n",
      "|   blocks.8                 |   7.1M               |\n",
      "|    blocks.8.norm1          |    1.5K              |\n",
      "|    blocks.8.attn           |    2.4M              |\n",
      "|    blocks.8.norm2          |    1.5K              |\n",
      "|    blocks.8.mlp            |    4.7M              |\n",
      "|   blocks.9                 |   7.1M               |\n",
      "|    blocks.9.norm1          |    1.5K              |\n",
      "|    blocks.9.attn           |    2.4M              |\n",
      "|    blocks.9.norm2          |    1.5K              |\n",
      "|    blocks.9.mlp            |    4.7M              |\n",
      "|   blocks.10                |   7.1M               |\n",
      "|    blocks.10.norm1         |    1.5K              |\n",
      "|    blocks.10.attn          |    2.4M              |\n",
      "|    blocks.10.norm2         |    1.5K              |\n",
      "|    blocks.10.mlp           |    4.7M              |\n",
      "|   blocks.11                |   7.1M               |\n",
      "|    blocks.11.norm1         |    1.5K              |\n",
      "|    blocks.11.attn          |    2.4M              |\n",
      "|    blocks.11.norm2         |    1.5K              |\n",
      "|    blocks.11.mlp           |    4.7M              |\n",
      "|  norm                      |  1.5K                |\n",
      "|   norm.weight              |   (768,)             |\n",
      "|   norm.bias                |   (768,)             |\n",
      "|  head                      |  0.8M                |\n",
      "|   head.weight              |   (1041, 768)        |\n",
      "|   head.bias                |   (1041,)            |\n",
      "|  trans_head                |  7.1M                |\n",
      "|   trans_head.norm1         |   1.5K               |\n",
      "|    trans_head.norm1.weight |    (768,)            |\n",
      "|    trans_head.norm1.bias   |    (768,)            |\n",
      "|   trans_head.attn          |   2.4M               |\n",
      "|    trans_head.attn.qkv     |    1.8M              |\n",
      "|    trans_head.attn.proj    |    0.6M              |\n",
      "|   trans_head.norm2         |   1.5K               |\n",
      "|    trans_head.norm2.weight |    (768,)            |\n",
      "|    trans_head.norm2.bias   |    (768,)            |\n",
      "|   trans_head.mlp           |   4.7M               |\n",
      "|    trans_head.mlp.fc1      |    2.4M              |\n",
      "|    trans_head.mlp.fc2      |    2.4M              |\n",
      "|  trans_norm                |  1.5K                |\n",
      "|   trans_norm.weight        |   (768,)             |\n",
      "|   trans_norm.bias          |   (768,)             |\n",
      "|  trans_fc                  |  0.8M                |\n",
      "|   trans_fc.weight          |   (1041, 768)        |\n",
      "|   trans_fc.bias            |   (1041,)            |\n",
      "|  trans_bn                  |  1.5K                |\n",
      "|   trans_bn.weight          |   (768,)             |\n",
      "|   trans_bn.bias            |   (768,)             |\n",
      "|  bn                        |  1.5K                |\n",
      "|   bn.weight                |   (768,)             |\n",
      "|   bn.bias                  |   (768,)             |\n"
     ]
    }
   ],
   "source": [
    "print(parameter_count_table(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Antelope fine-tuning')\n",
    "parser.add_argument('--config_file', default='', help='path to config file', type=str)\n",
    "parser.add_argument('opts', help='modify config options using the command-line', default=None, nargs=argparse.REMAINDER)\n",
    "args = parser.parse_args(args=['--config_file', 'configs/finetune/MSMT17/mae_inet_lup_vitb_ep800_ratio_optimized/lem.yaml'])\n",
    "cfg.merge_from_file(args.config_file)\n",
    "cfg.merge_from_list(args.opts)\n",
    "cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, num_classes, num_queries = initialize_data_loader(cfg)\n",
    "model, criterion, optimizer, scaler = initialize_model(cfg, num_classes, device_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyz/miniconda3/lib/python3.9/site-packages/timm/models/vision_transformer.py:202: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
      "Unsupported operator aten::add encountered 25 time(s)\n",
      "Unsupported operator aten::div encountered 12 time(s)\n",
      "Unsupported operator aten::mul encountered 34 time(s)\n",
      "Unsupported operator aten::softmax encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::bernoulli_ encountered 22 time(s)\n",
      "Unsupported operator aten::div_ encountered 22 time(s)\n",
      "Unsupported operator aten::add_ encountered 15 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745756999680\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "flops = FlopCountAnalysis(model, batch[0].cuda())\n",
    "print(flops.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| name                       | #elements or shape   |\n",
      "|:---------------------------|:---------------------|\n",
      "| model                      | 96.6M                |\n",
      "|  cls_token                 |  (1, 1, 768)         |\n",
      "|  pos_embed                 |  (1, 129, 768)       |\n",
      "|  patch_embed               |  0.6M                |\n",
      "|   patch_embed.proj         |   0.6M               |\n",
      "|    patch_embed.proj.weight |    (768, 3, 16, 16)  |\n",
      "|    patch_embed.proj.bias   |    (768,)            |\n",
      "|  blocks                    |  85.1M               |\n",
      "|   blocks.0                 |   7.1M               |\n",
      "|    blocks.0.norm1          |    1.5K              |\n",
      "|    blocks.0.attn           |    2.4M              |\n",
      "|    blocks.0.norm2          |    1.5K              |\n",
      "|    blocks.0.mlp            |    4.7M              |\n",
      "|   blocks.1                 |   7.1M               |\n",
      "|    blocks.1.norm1          |    1.5K              |\n",
      "|    blocks.1.attn           |    2.4M              |\n",
      "|    blocks.1.norm2          |    1.5K              |\n",
      "|    blocks.1.mlp            |    4.7M              |\n",
      "|   blocks.2                 |   7.1M               |\n",
      "|    blocks.2.norm1          |    1.5K              |\n",
      "|    blocks.2.attn           |    2.4M              |\n",
      "|    blocks.2.norm2          |    1.5K              |\n",
      "|    blocks.2.mlp            |    4.7M              |\n",
      "|   blocks.3                 |   7.1M               |\n",
      "|    blocks.3.norm1          |    1.5K              |\n",
      "|    blocks.3.attn           |    2.4M              |\n",
      "|    blocks.3.norm2          |    1.5K              |\n",
      "|    blocks.3.mlp            |    4.7M              |\n",
      "|   blocks.4                 |   7.1M               |\n",
      "|    blocks.4.norm1          |    1.5K              |\n",
      "|    blocks.4.attn           |    2.4M              |\n",
      "|    blocks.4.norm2          |    1.5K              |\n",
      "|    blocks.4.mlp            |    4.7M              |\n",
      "|   blocks.5                 |   7.1M               |\n",
      "|    blocks.5.norm1          |    1.5K              |\n",
      "|    blocks.5.attn           |    2.4M              |\n",
      "|    blocks.5.norm2          |    1.5K              |\n",
      "|    blocks.5.mlp            |    4.7M              |\n",
      "|   blocks.6                 |   7.1M               |\n",
      "|    blocks.6.norm1          |    1.5K              |\n",
      "|    blocks.6.attn           |    2.4M              |\n",
      "|    blocks.6.norm2          |    1.5K              |\n",
      "|    blocks.6.mlp            |    4.7M              |\n",
      "|   blocks.7                 |   7.1M               |\n",
      "|    blocks.7.norm1          |    1.5K              |\n",
      "|    blocks.7.attn           |    2.4M              |\n",
      "|    blocks.7.norm2          |    1.5K              |\n",
      "|    blocks.7.mlp            |    4.7M              |\n",
      "|   blocks.8                 |   7.1M               |\n",
      "|    blocks.8.norm1          |    1.5K              |\n",
      "|    blocks.8.attn           |    2.4M              |\n",
      "|    blocks.8.norm2          |    1.5K              |\n",
      "|    blocks.8.mlp            |    4.7M              |\n",
      "|   blocks.9                 |   7.1M               |\n",
      "|    blocks.9.norm1          |    1.5K              |\n",
      "|    blocks.9.attn           |    2.4M              |\n",
      "|    blocks.9.norm2          |    1.5K              |\n",
      "|    blocks.9.mlp            |    4.7M              |\n",
      "|   blocks.10                |   7.1M               |\n",
      "|    blocks.10.norm1         |    1.5K              |\n",
      "|    blocks.10.attn          |    2.4M              |\n",
      "|    blocks.10.norm2         |    1.5K              |\n",
      "|    blocks.10.mlp           |    4.7M              |\n",
      "|   blocks.11                |   7.1M               |\n",
      "|    blocks.11.norm1         |    1.5K              |\n",
      "|    blocks.11.attn          |    2.4M              |\n",
      "|    blocks.11.norm2         |    1.5K              |\n",
      "|    blocks.11.mlp           |    4.7M              |\n",
      "|  norm                      |  1.5K                |\n",
      "|   norm.weight              |   (768,)             |\n",
      "|   norm.bias                |   (768,)             |\n",
      "|  head                      |  0.8M                |\n",
      "|   head.weight              |   (1041, 768)        |\n",
      "|   head.bias                |   (1041,)            |\n",
      "|  conv_head                 |  8.4M                |\n",
      "|   conv_head.0              |   3.4M               |\n",
      "|    conv_head.0.conv1       |    0.3M              |\n",
      "|    conv_head.0.bn1         |    0.8K              |\n",
      "|    conv_head.0.conv2       |    1.3M              |\n",
      "|    conv_head.0.bn2         |    0.8K              |\n",
      "|    conv_head.0.conv3       |    0.6M              |\n",
      "|    conv_head.0.bn3         |    3.1K              |\n",
      "|    conv_head.0.downsample  |    1.2M              |\n",
      "|   conv_head.1              |   2.5M               |\n",
      "|    conv_head.1.conv1       |    0.6M              |\n",
      "|    conv_head.1.bn1         |    0.8K              |\n",
      "|    conv_head.1.conv2       |    1.3M              |\n",
      "|    conv_head.1.bn2         |    0.8K              |\n",
      "|    conv_head.1.conv3       |    0.6M              |\n",
      "|    conv_head.1.bn3         |    3.1K              |\n",
      "|   conv_head.2              |   2.5M               |\n",
      "|    conv_head.2.conv1       |    0.6M              |\n",
      "|    conv_head.2.bn1         |    0.8K              |\n",
      "|    conv_head.2.conv2       |    1.3M              |\n",
      "|    conv_head.2.bn2         |    0.8K              |\n",
      "|    conv_head.2.conv3       |    0.6M              |\n",
      "|    conv_head.2.bn3         |    3.1K              |\n",
      "|  conv_fc                   |  1.6M                |\n",
      "|   conv_fc.weight           |   (1041, 1536)       |\n",
      "|   conv_fc.bias             |   (1041,)            |\n",
      "|  conv_fc_norm              |  3.1K                |\n",
      "|   conv_fc_norm.weight      |   (1536,)            |\n",
      "|   conv_fc_norm.bias        |   (1536,)            |\n",
      "|  conv_bn                   |  3.1K                |\n",
      "|   conv_bn.weight           |   (1536,)            |\n",
      "|   conv_bn.bias             |   (1536,)            |\n",
      "|  bn                        |  1.5K                |\n",
      "|   bn.weight                |   (768,)             |\n",
      "|   bn.bias                  |   (768,)             |\n"
     ]
    }
   ],
   "source": [
    "print(parameter_count_table(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Antelope fine-tuning')\n",
    "parser.add_argument('--config_file', default='', help='path to config file', type=str)\n",
    "parser.add_argument('opts', help='modify config options using the command-line', default=None, nargs=argparse.REMAINDER)\n",
    "args = parser.parse_args(args=['--config_file', 'configs/finetune/MSMT17/mae_inet_lup_vitb_ep800_ratio_optimized/lem_plus.yaml'])\n",
    "cfg.merge_from_file(args.config_file)\n",
    "cfg.merge_from_list(args.opts)\n",
    "cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, num_classes, num_queries = initialize_data_loader(cfg)\n",
    "model, criterion, optimizer, scaler = initialize_model(cfg, num_classes, device_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyz/miniconda3/lib/python3.9/site-packages/timm/models/vision_transformer.py:202: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
      "Unsupported operator aten::add encountered 25 time(s)\n",
      "Unsupported operator aten::div encountered 12 time(s)\n",
      "Unsupported operator aten::mul encountered 34 time(s)\n",
      "Unsupported operator aten::softmax encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::bernoulli_ encountered 22 time(s)\n",
      "Unsupported operator aten::div_ encountered 22 time(s)\n",
      "Unsupported operator aten::add_ encountered 9 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "881330798592\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "flops = FlopCountAnalysis(model, batch[0].cuda())\n",
    "print(flops.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| name                       | #elements or shape   |\n",
      "|:---------------------------|:---------------------|\n",
      "| model                      | 0.2G                 |\n",
      "|  cls_token                 |  (1, 1, 768)         |\n",
      "|  pos_embed                 |  (1, 129, 768)       |\n",
      "|  patch_embed               |  0.6M                |\n",
      "|   patch_embed.proj         |   0.6M               |\n",
      "|    patch_embed.proj.weight |    (768, 3, 16, 16)  |\n",
      "|    patch_embed.proj.bias   |    (768,)            |\n",
      "|  blocks                    |  85.1M               |\n",
      "|   blocks.0                 |   7.1M               |\n",
      "|    blocks.0.norm1          |    1.5K              |\n",
      "|    blocks.0.attn           |    2.4M              |\n",
      "|    blocks.0.norm2          |    1.5K              |\n",
      "|    blocks.0.mlp            |    4.7M              |\n",
      "|   blocks.1                 |   7.1M               |\n",
      "|    blocks.1.norm1          |    1.5K              |\n",
      "|    blocks.1.attn           |    2.4M              |\n",
      "|    blocks.1.norm2          |    1.5K              |\n",
      "|    blocks.1.mlp            |    4.7M              |\n",
      "|   blocks.2                 |   7.1M               |\n",
      "|    blocks.2.norm1          |    1.5K              |\n",
      "|    blocks.2.attn           |    2.4M              |\n",
      "|    blocks.2.norm2          |    1.5K              |\n",
      "|    blocks.2.mlp            |    4.7M              |\n",
      "|   blocks.3                 |   7.1M               |\n",
      "|    blocks.3.norm1          |    1.5K              |\n",
      "|    blocks.3.attn           |    2.4M              |\n",
      "|    blocks.3.norm2          |    1.5K              |\n",
      "|    blocks.3.mlp            |    4.7M              |\n",
      "|   blocks.4                 |   7.1M               |\n",
      "|    blocks.4.norm1          |    1.5K              |\n",
      "|    blocks.4.attn           |    2.4M              |\n",
      "|    blocks.4.norm2          |    1.5K              |\n",
      "|    blocks.4.mlp            |    4.7M              |\n",
      "|   blocks.5                 |   7.1M               |\n",
      "|    blocks.5.norm1          |    1.5K              |\n",
      "|    blocks.5.attn           |    2.4M              |\n",
      "|    blocks.5.norm2          |    1.5K              |\n",
      "|    blocks.5.mlp            |    4.7M              |\n",
      "|   blocks.6                 |   7.1M               |\n",
      "|    blocks.6.norm1          |    1.5K              |\n",
      "|    blocks.6.attn           |    2.4M              |\n",
      "|    blocks.6.norm2          |    1.5K              |\n",
      "|    blocks.6.mlp            |    4.7M              |\n",
      "|   blocks.7                 |   7.1M               |\n",
      "|    blocks.7.norm1          |    1.5K              |\n",
      "|    blocks.7.attn           |    2.4M              |\n",
      "|    blocks.7.norm2          |    1.5K              |\n",
      "|    blocks.7.mlp            |    4.7M              |\n",
      "|   blocks.8                 |   7.1M               |\n",
      "|    blocks.8.norm1          |    1.5K              |\n",
      "|    blocks.8.attn           |    2.4M              |\n",
      "|    blocks.8.norm2          |    1.5K              |\n",
      "|    blocks.8.mlp            |    4.7M              |\n",
      "|   blocks.9                 |   7.1M               |\n",
      "|    blocks.9.norm1          |    1.5K              |\n",
      "|    blocks.9.attn           |    2.4M              |\n",
      "|    blocks.9.norm2          |    1.5K              |\n",
      "|    blocks.9.mlp            |    4.7M              |\n",
      "|   blocks.10                |   7.1M               |\n",
      "|    blocks.10.norm1         |    1.5K              |\n",
      "|    blocks.10.attn          |    2.4M              |\n",
      "|    blocks.10.norm2         |    1.5K              |\n",
      "|    blocks.10.mlp           |    4.7M              |\n",
      "|   blocks.11                |   7.1M               |\n",
      "|    blocks.11.norm1         |    1.5K              |\n",
      "|    blocks.11.attn          |    2.4M              |\n",
      "|    blocks.11.norm2         |    1.5K              |\n",
      "|    blocks.11.mlp           |    4.7M              |\n",
      "|  norm                      |  1.5K                |\n",
      "|   norm.weight              |   (768,)             |\n",
      "|   norm.bias                |   (768,)             |\n",
      "|  head                      |  0.8M                |\n",
      "|   head.weight              |   (1041, 768)        |\n",
      "|   head.bias                |   (1041,)            |\n",
      "|  conv_head                 |  75.5M               |\n",
      "|   conv_head.0              |   33.0M              |\n",
      "|    conv_head.0.conv1       |    10.6M             |\n",
      "|    conv_head.0.bn1         |    3.1K              |\n",
      "|    conv_head.0.conv2       |    21.2M             |\n",
      "|    conv_head.0.bn2         |    3.1K              |\n",
      "|    conv_head.0.downsample  |    1.2M              |\n",
      "|   conv_head.1              |   42.5M              |\n",
      "|    conv_head.1.conv1       |    21.2M             |\n",
      "|    conv_head.1.bn1         |    3.1K              |\n",
      "|    conv_head.1.conv2       |    21.2M             |\n",
      "|    conv_head.1.bn2         |    3.1K              |\n",
      "|  conv_fc                   |  1.6M                |\n",
      "|   conv_fc.weight           |   (1041, 1536)       |\n",
      "|   conv_fc.bias             |   (1041,)            |\n",
      "|  conv_fc_norm              |  3.1K                |\n",
      "|   conv_fc_norm.weight      |   (1536,)            |\n",
      "|   conv_fc_norm.bias        |   (1536,)            |\n",
      "|  conv_bn                   |  3.1K                |\n",
      "|   conv_bn.weight           |   (1536,)            |\n",
      "|   conv_bn.bias             |   (1536,)            |\n",
      "|  bn                        |  1.5K                |\n",
      "|   bn.weight                |   (768,)             |\n",
      "|   bn.bias                  |   (768,)             |\n"
     ]
    }
   ],
   "source": [
    "print(parameter_count_table(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'': 163667490,\n",
       "             'cls_token': 768,\n",
       "             'pos_embed': 99072,\n",
       "             'patch_embed': 590592,\n",
       "             'patch_embed.proj': 590592,\n",
       "             'patch_embed.proj.weight': 589824,\n",
       "             'patch_embed.proj.bias': 768,\n",
       "             'blocks': 85054464,\n",
       "             'blocks.0': 7087872,\n",
       "             'blocks.0.norm1': 1536,\n",
       "             'blocks.0.norm1.weight': 768,\n",
       "             'blocks.0.norm1.bias': 768,\n",
       "             'blocks.0.attn': 2362368,\n",
       "             'blocks.0.attn.qkv': 1771776,\n",
       "             'blocks.0.attn.qkv.weight': 1769472,\n",
       "             'blocks.0.attn.qkv.bias': 2304,\n",
       "             'blocks.0.attn.proj': 590592,\n",
       "             'blocks.0.attn.proj.weight': 589824,\n",
       "             'blocks.0.attn.proj.bias': 768,\n",
       "             'blocks.0.norm2': 1536,\n",
       "             'blocks.0.norm2.weight': 768,\n",
       "             'blocks.0.norm2.bias': 768,\n",
       "             'blocks.0.mlp': 4722432,\n",
       "             'blocks.0.mlp.fc1': 2362368,\n",
       "             'blocks.0.mlp.fc1.weight': 2359296,\n",
       "             'blocks.0.mlp.fc1.bias': 3072,\n",
       "             'blocks.0.mlp.fc2': 2360064,\n",
       "             'blocks.0.mlp.fc2.weight': 2359296,\n",
       "             'blocks.0.mlp.fc2.bias': 768,\n",
       "             'blocks.1': 7087872,\n",
       "             'blocks.1.norm1': 1536,\n",
       "             'blocks.1.norm1.weight': 768,\n",
       "             'blocks.1.norm1.bias': 768,\n",
       "             'blocks.1.attn': 2362368,\n",
       "             'blocks.1.attn.qkv': 1771776,\n",
       "             'blocks.1.attn.qkv.weight': 1769472,\n",
       "             'blocks.1.attn.qkv.bias': 2304,\n",
       "             'blocks.1.attn.proj': 590592,\n",
       "             'blocks.1.attn.proj.weight': 589824,\n",
       "             'blocks.1.attn.proj.bias': 768,\n",
       "             'blocks.1.norm2': 1536,\n",
       "             'blocks.1.norm2.weight': 768,\n",
       "             'blocks.1.norm2.bias': 768,\n",
       "             'blocks.1.mlp': 4722432,\n",
       "             'blocks.1.mlp.fc1': 2362368,\n",
       "             'blocks.1.mlp.fc1.weight': 2359296,\n",
       "             'blocks.1.mlp.fc1.bias': 3072,\n",
       "             'blocks.1.mlp.fc2': 2360064,\n",
       "             'blocks.1.mlp.fc2.weight': 2359296,\n",
       "             'blocks.1.mlp.fc2.bias': 768,\n",
       "             'blocks.2': 7087872,\n",
       "             'blocks.2.norm1': 1536,\n",
       "             'blocks.2.norm1.weight': 768,\n",
       "             'blocks.2.norm1.bias': 768,\n",
       "             'blocks.2.attn': 2362368,\n",
       "             'blocks.2.attn.qkv': 1771776,\n",
       "             'blocks.2.attn.qkv.weight': 1769472,\n",
       "             'blocks.2.attn.qkv.bias': 2304,\n",
       "             'blocks.2.attn.proj': 590592,\n",
       "             'blocks.2.attn.proj.weight': 589824,\n",
       "             'blocks.2.attn.proj.bias': 768,\n",
       "             'blocks.2.norm2': 1536,\n",
       "             'blocks.2.norm2.weight': 768,\n",
       "             'blocks.2.norm2.bias': 768,\n",
       "             'blocks.2.mlp': 4722432,\n",
       "             'blocks.2.mlp.fc1': 2362368,\n",
       "             'blocks.2.mlp.fc1.weight': 2359296,\n",
       "             'blocks.2.mlp.fc1.bias': 3072,\n",
       "             'blocks.2.mlp.fc2': 2360064,\n",
       "             'blocks.2.mlp.fc2.weight': 2359296,\n",
       "             'blocks.2.mlp.fc2.bias': 768,\n",
       "             'blocks.3': 7087872,\n",
       "             'blocks.3.norm1': 1536,\n",
       "             'blocks.3.norm1.weight': 768,\n",
       "             'blocks.3.norm1.bias': 768,\n",
       "             'blocks.3.attn': 2362368,\n",
       "             'blocks.3.attn.qkv': 1771776,\n",
       "             'blocks.3.attn.qkv.weight': 1769472,\n",
       "             'blocks.3.attn.qkv.bias': 2304,\n",
       "             'blocks.3.attn.proj': 590592,\n",
       "             'blocks.3.attn.proj.weight': 589824,\n",
       "             'blocks.3.attn.proj.bias': 768,\n",
       "             'blocks.3.norm2': 1536,\n",
       "             'blocks.3.norm2.weight': 768,\n",
       "             'blocks.3.norm2.bias': 768,\n",
       "             'blocks.3.mlp': 4722432,\n",
       "             'blocks.3.mlp.fc1': 2362368,\n",
       "             'blocks.3.mlp.fc1.weight': 2359296,\n",
       "             'blocks.3.mlp.fc1.bias': 3072,\n",
       "             'blocks.3.mlp.fc2': 2360064,\n",
       "             'blocks.3.mlp.fc2.weight': 2359296,\n",
       "             'blocks.3.mlp.fc2.bias': 768,\n",
       "             'blocks.4': 7087872,\n",
       "             'blocks.4.norm1': 1536,\n",
       "             'blocks.4.norm1.weight': 768,\n",
       "             'blocks.4.norm1.bias': 768,\n",
       "             'blocks.4.attn': 2362368,\n",
       "             'blocks.4.attn.qkv': 1771776,\n",
       "             'blocks.4.attn.qkv.weight': 1769472,\n",
       "             'blocks.4.attn.qkv.bias': 2304,\n",
       "             'blocks.4.attn.proj': 590592,\n",
       "             'blocks.4.attn.proj.weight': 589824,\n",
       "             'blocks.4.attn.proj.bias': 768,\n",
       "             'blocks.4.norm2': 1536,\n",
       "             'blocks.4.norm2.weight': 768,\n",
       "             'blocks.4.norm2.bias': 768,\n",
       "             'blocks.4.mlp': 4722432,\n",
       "             'blocks.4.mlp.fc1': 2362368,\n",
       "             'blocks.4.mlp.fc1.weight': 2359296,\n",
       "             'blocks.4.mlp.fc1.bias': 3072,\n",
       "             'blocks.4.mlp.fc2': 2360064,\n",
       "             'blocks.4.mlp.fc2.weight': 2359296,\n",
       "             'blocks.4.mlp.fc2.bias': 768,\n",
       "             'blocks.5': 7087872,\n",
       "             'blocks.5.norm1': 1536,\n",
       "             'blocks.5.norm1.weight': 768,\n",
       "             'blocks.5.norm1.bias': 768,\n",
       "             'blocks.5.attn': 2362368,\n",
       "             'blocks.5.attn.qkv': 1771776,\n",
       "             'blocks.5.attn.qkv.weight': 1769472,\n",
       "             'blocks.5.attn.qkv.bias': 2304,\n",
       "             'blocks.5.attn.proj': 590592,\n",
       "             'blocks.5.attn.proj.weight': 589824,\n",
       "             'blocks.5.attn.proj.bias': 768,\n",
       "             'blocks.5.norm2': 1536,\n",
       "             'blocks.5.norm2.weight': 768,\n",
       "             'blocks.5.norm2.bias': 768,\n",
       "             'blocks.5.mlp': 4722432,\n",
       "             'blocks.5.mlp.fc1': 2362368,\n",
       "             'blocks.5.mlp.fc1.weight': 2359296,\n",
       "             'blocks.5.mlp.fc1.bias': 3072,\n",
       "             'blocks.5.mlp.fc2': 2360064,\n",
       "             'blocks.5.mlp.fc2.weight': 2359296,\n",
       "             'blocks.5.mlp.fc2.bias': 768,\n",
       "             'blocks.6': 7087872,\n",
       "             'blocks.6.norm1': 1536,\n",
       "             'blocks.6.norm1.weight': 768,\n",
       "             'blocks.6.norm1.bias': 768,\n",
       "             'blocks.6.attn': 2362368,\n",
       "             'blocks.6.attn.qkv': 1771776,\n",
       "             'blocks.6.attn.qkv.weight': 1769472,\n",
       "             'blocks.6.attn.qkv.bias': 2304,\n",
       "             'blocks.6.attn.proj': 590592,\n",
       "             'blocks.6.attn.proj.weight': 589824,\n",
       "             'blocks.6.attn.proj.bias': 768,\n",
       "             'blocks.6.norm2': 1536,\n",
       "             'blocks.6.norm2.weight': 768,\n",
       "             'blocks.6.norm2.bias': 768,\n",
       "             'blocks.6.mlp': 4722432,\n",
       "             'blocks.6.mlp.fc1': 2362368,\n",
       "             'blocks.6.mlp.fc1.weight': 2359296,\n",
       "             'blocks.6.mlp.fc1.bias': 3072,\n",
       "             'blocks.6.mlp.fc2': 2360064,\n",
       "             'blocks.6.mlp.fc2.weight': 2359296,\n",
       "             'blocks.6.mlp.fc2.bias': 768,\n",
       "             'blocks.7': 7087872,\n",
       "             'blocks.7.norm1': 1536,\n",
       "             'blocks.7.norm1.weight': 768,\n",
       "             'blocks.7.norm1.bias': 768,\n",
       "             'blocks.7.attn': 2362368,\n",
       "             'blocks.7.attn.qkv': 1771776,\n",
       "             'blocks.7.attn.qkv.weight': 1769472,\n",
       "             'blocks.7.attn.qkv.bias': 2304,\n",
       "             'blocks.7.attn.proj': 590592,\n",
       "             'blocks.7.attn.proj.weight': 589824,\n",
       "             'blocks.7.attn.proj.bias': 768,\n",
       "             'blocks.7.norm2': 1536,\n",
       "             'blocks.7.norm2.weight': 768,\n",
       "             'blocks.7.norm2.bias': 768,\n",
       "             'blocks.7.mlp': 4722432,\n",
       "             'blocks.7.mlp.fc1': 2362368,\n",
       "             'blocks.7.mlp.fc1.weight': 2359296,\n",
       "             'blocks.7.mlp.fc1.bias': 3072,\n",
       "             'blocks.7.mlp.fc2': 2360064,\n",
       "             'blocks.7.mlp.fc2.weight': 2359296,\n",
       "             'blocks.7.mlp.fc2.bias': 768,\n",
       "             'blocks.8': 7087872,\n",
       "             'blocks.8.norm1': 1536,\n",
       "             'blocks.8.norm1.weight': 768,\n",
       "             'blocks.8.norm1.bias': 768,\n",
       "             'blocks.8.attn': 2362368,\n",
       "             'blocks.8.attn.qkv': 1771776,\n",
       "             'blocks.8.attn.qkv.weight': 1769472,\n",
       "             'blocks.8.attn.qkv.bias': 2304,\n",
       "             'blocks.8.attn.proj': 590592,\n",
       "             'blocks.8.attn.proj.weight': 589824,\n",
       "             'blocks.8.attn.proj.bias': 768,\n",
       "             'blocks.8.norm2': 1536,\n",
       "             'blocks.8.norm2.weight': 768,\n",
       "             'blocks.8.norm2.bias': 768,\n",
       "             'blocks.8.mlp': 4722432,\n",
       "             'blocks.8.mlp.fc1': 2362368,\n",
       "             'blocks.8.mlp.fc1.weight': 2359296,\n",
       "             'blocks.8.mlp.fc1.bias': 3072,\n",
       "             'blocks.8.mlp.fc2': 2360064,\n",
       "             'blocks.8.mlp.fc2.weight': 2359296,\n",
       "             'blocks.8.mlp.fc2.bias': 768,\n",
       "             'blocks.9': 7087872,\n",
       "             'blocks.9.norm1': 1536,\n",
       "             'blocks.9.norm1.weight': 768,\n",
       "             'blocks.9.norm1.bias': 768,\n",
       "             'blocks.9.attn': 2362368,\n",
       "             'blocks.9.attn.qkv': 1771776,\n",
       "             'blocks.9.attn.qkv.weight': 1769472,\n",
       "             'blocks.9.attn.qkv.bias': 2304,\n",
       "             'blocks.9.attn.proj': 590592,\n",
       "             'blocks.9.attn.proj.weight': 589824,\n",
       "             'blocks.9.attn.proj.bias': 768,\n",
       "             'blocks.9.norm2': 1536,\n",
       "             'blocks.9.norm2.weight': 768,\n",
       "             'blocks.9.norm2.bias': 768,\n",
       "             'blocks.9.mlp': 4722432,\n",
       "             'blocks.9.mlp.fc1': 2362368,\n",
       "             'blocks.9.mlp.fc1.weight': 2359296,\n",
       "             'blocks.9.mlp.fc1.bias': 3072,\n",
       "             'blocks.9.mlp.fc2': 2360064,\n",
       "             'blocks.9.mlp.fc2.weight': 2359296,\n",
       "             'blocks.9.mlp.fc2.bias': 768,\n",
       "             'blocks.10': 7087872,\n",
       "             'blocks.10.norm1': 1536,\n",
       "             'blocks.10.norm1.weight': 768,\n",
       "             'blocks.10.norm1.bias': 768,\n",
       "             'blocks.10.attn': 2362368,\n",
       "             'blocks.10.attn.qkv': 1771776,\n",
       "             'blocks.10.attn.qkv.weight': 1769472,\n",
       "             'blocks.10.attn.qkv.bias': 2304,\n",
       "             'blocks.10.attn.proj': 590592,\n",
       "             'blocks.10.attn.proj.weight': 589824,\n",
       "             'blocks.10.attn.proj.bias': 768,\n",
       "             'blocks.10.norm2': 1536,\n",
       "             'blocks.10.norm2.weight': 768,\n",
       "             'blocks.10.norm2.bias': 768,\n",
       "             'blocks.10.mlp': 4722432,\n",
       "             'blocks.10.mlp.fc1': 2362368,\n",
       "             'blocks.10.mlp.fc1.weight': 2359296,\n",
       "             'blocks.10.mlp.fc1.bias': 3072,\n",
       "             'blocks.10.mlp.fc2': 2360064,\n",
       "             'blocks.10.mlp.fc2.weight': 2359296,\n",
       "             'blocks.10.mlp.fc2.bias': 768,\n",
       "             'blocks.11': 7087872,\n",
       "             'blocks.11.norm1': 1536,\n",
       "             'blocks.11.norm1.weight': 768,\n",
       "             'blocks.11.norm1.bias': 768,\n",
       "             'blocks.11.attn': 2362368,\n",
       "             'blocks.11.attn.qkv': 1771776,\n",
       "             'blocks.11.attn.qkv.weight': 1769472,\n",
       "             'blocks.11.attn.qkv.bias': 2304,\n",
       "             'blocks.11.attn.proj': 590592,\n",
       "             'blocks.11.attn.proj.weight': 589824,\n",
       "             'blocks.11.attn.proj.bias': 768,\n",
       "             'blocks.11.norm2': 1536,\n",
       "             'blocks.11.norm2.weight': 768,\n",
       "             'blocks.11.norm2.bias': 768,\n",
       "             'blocks.11.mlp': 4722432,\n",
       "             'blocks.11.mlp.fc1': 2362368,\n",
       "             'blocks.11.mlp.fc1.weight': 2359296,\n",
       "             'blocks.11.mlp.fc1.bias': 3072,\n",
       "             'blocks.11.mlp.fc2': 2360064,\n",
       "             'blocks.11.mlp.fc2.weight': 2359296,\n",
       "             'blocks.11.mlp.fc2.bias': 768,\n",
       "             'norm': 1536,\n",
       "             'norm.weight': 768,\n",
       "             'norm.bias': 768,\n",
       "             'head': 800529,\n",
       "             'head.weight': 799488,\n",
       "             'head.bias': 1041,\n",
       "             'conv_head': 75512832,\n",
       "             'conv_head.0': 33039360,\n",
       "             'conv_head.0.conv1': 10616832,\n",
       "             'conv_head.0.conv1.weight': 10616832,\n",
       "             'conv_head.0.bn1': 3072,\n",
       "             'conv_head.0.bn1.weight': 1536,\n",
       "             'conv_head.0.bn1.bias': 1536,\n",
       "             'conv_head.0.conv2': 21233664,\n",
       "             'conv_head.0.conv2.weight': 21233664,\n",
       "             'conv_head.0.bn2': 3072,\n",
       "             'conv_head.0.bn2.weight': 1536,\n",
       "             'conv_head.0.bn2.bias': 1536,\n",
       "             'conv_head.0.downsample': 1182720,\n",
       "             'conv_head.0.downsample.0': 1179648,\n",
       "             'conv_head.0.downsample.0.weight': 1179648,\n",
       "             'conv_head.0.downsample.1': 3072,\n",
       "             'conv_head.0.downsample.1.weight': 1536,\n",
       "             'conv_head.0.downsample.1.bias': 1536,\n",
       "             'conv_head.1': 42473472,\n",
       "             'conv_head.1.conv1': 21233664,\n",
       "             'conv_head.1.conv1.weight': 21233664,\n",
       "             'conv_head.1.bn1': 3072,\n",
       "             'conv_head.1.bn1.weight': 1536,\n",
       "             'conv_head.1.bn1.bias': 1536,\n",
       "             'conv_head.1.conv2': 21233664,\n",
       "             'conv_head.1.conv2.weight': 21233664,\n",
       "             'conv_head.1.bn2': 3072,\n",
       "             'conv_head.1.bn2.weight': 1536,\n",
       "             'conv_head.1.bn2.bias': 1536,\n",
       "             'conv_fc': 1600017,\n",
       "             'conv_fc.weight': 1598976,\n",
       "             'conv_fc.bias': 1041,\n",
       "             'conv_fc_norm': 3072,\n",
       "             'conv_fc_norm.weight': 1536,\n",
       "             'conv_fc_norm.bias': 1536,\n",
       "             'conv_bn': 3072,\n",
       "             'conv_bn.weight': 1536,\n",
       "             'conv_bn.bias': 1536,\n",
       "             'bn': 1536,\n",
       "             'bn.weight': 768,\n",
       "             'bn.bias': 768})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fvcore.nn import parameter_count\n",
    "parameter_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92ce6405c4fca5e57313421a86d09281a436329c0724ea0d2004605177eadcb1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
